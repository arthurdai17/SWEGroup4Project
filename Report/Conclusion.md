# CONCLUSION 

### Project Management and Team Strategy: 

Looking back at the actions taken in this project, there are several things that we would do differently as a team if we were to repeat this project again. The first of these relates to project management. As previously stated, the team initially decided to implement a Scrum approach to the AGILE methodology but realised a Kanban approach would be more appropriate. The piece that was missing in both cases was a formal way of tracking the overall progress of the project. Team meetings provided the immediate set of priorities however did not capture the project as a whole. OneDrive and Git were also useful tools but only provided the current state of the project with no way to view all completed tasks (in a format that was useful) as well as any future targets to be met. If this project was to be repeated, the use of a tool such as a Kanban board would be very beneficial. The benefit of a Kanban board is that it facilitates team collaboration while also providing both an overview of the entire project as well as a snapshot of the current state of the project. Each team member would be able to update their individual progress on the board, which could then be viewed in the context of the entire project by the rest of the team.  

Another issue that we faced relating to project-management is that although files were regularly being uploaded to OneDrive, the folder itself was being checked too infrequently by team members. Given that this is where all the latest updates were being captured, it is important that all team members are keeping up to date with any changes or new information added to the folder on a regular basis. A similar issue arose with Git in that commits were happening far too infrequently for it to be useful. When working in teams, it is important that members are aware of the current state of the project at any given time. This is important because any changes that need to be made can be caught early rather than trying to complete sets of tasks that may now be out of date as a result.  

As a team we understood early on the benefit of implementing the AGILE methodology as opposed to the (generally) outdated ‘waterfall’ approach of the past. However, it is clear that although there are many interpretations of the AGILE methodology, it is not essential to choose just one. Every project and every team is different and it is perhaps optimistic to try to force a single approach onto a given project or team without considering its implications. We came to realise that rather than choosing between either Scrum or Kanban, a hybrid of both approaches would be the best for this project and team. For much of the project, the Kanban framework was used. However, we later decided to switch the “roles” structure back more of a Scrum approach, where each team member had a distinct role in the project. This provided the flexibility the team needed to voice new ideas and modify parts of the application as well as provide a sense of individual responsibility that was necessary to promote proactivity.  

Redefining the team member roles was a critical step to ensure the project kept moving forward in a way that would allow us to deliver the application in the specified timeframe. Some major changes were made to ensure this was the case. Through further meetings it was brought to our attention that some team members felt uncomfortable assisting with the writing of the report due to the fact that English was not their first language – something which we had initially overlooked. It also became apparent that certain members were struggling with the technical side of the project but were able to provide non-technical assistance such as content and feedback collection. Looking back on the project, it is clear that some of these issues could have been foreshadowed and if we were to repeat the project again in the future, it would be prudent to conduct a more thorough analysis of the team’s strategy upfront rather than making too many assumptions in the early stages of the project.  

### Reflection of Project Success: 

In this section, we will evaluate the extent to which the project objectives have been met. 

_**Objective 1: To bring people together by creating an application that helps identify the cultural similarities they have with one-another**_ 

Based on our final product, it can be said that this objective has not been explicitly met. While the application does teach people about different cultures, it currently has no feature to explicitly show the similarities between one culture and another. That said, participants seemed pleased with the learning experience saying on average that they had a better understanding of the culture they were presented with. This is also backed by the average score awarded for the learning experience which was a 4.1 out of the potential 5. As mentioned in the Active Recall section, content that has been committed to memory is much more likely to be applied in real-world situations. An argument could therefore be made that if Objective 3 has been met, it is possible that participants could extrapolate the knowledge they have gained from using the application and make their own connections between the different cultures they have learnt about.  

_**Objective 2: To deliver information in a way that is interactive and engaging so that users enjoy using the application (and are likely to continue using it)**_ 

Based on the data collected, it can be said that this objective has been partially met. From the quantitative data, we can see that the application scored a 4.2/5 on average across all sections. Users also awarded the application a score of 4.1 in the ‘Fun to Use’ category, which supports the objective to ensure the content was delivered in a way that is engaging for the users. However, when users were asked to provide qualitative feedback, 3 out of the 15 felt that there was too much content being provided once the questions were answered. 2 participants also felt that the quiz needed to be more exciting by introducing an element of competition to it and a further two said that they would like to see more variety in the activities offered rather than offering just a quiz format.  

What we have noticed is that there appears to be some conflict between the quantitative data gathered and the qualitative data. The quantitative data shows that participants are overall happy with the application and find it both engaging and effective. On the other hand, from the qualitative feedback, participants have outlined improvements that can be made in areas that appeared to have scored highly in the quantitative section. There are a number of potential reasons for this discrepancy. For instance, it could be that the participants felt that they had to write about something to improve in the qualitative section of the questionnaire rather than leaving that section blank. This may have forced them to write something, even if they don’t necessarily fully believe it to be true.  

Another reason could be that the 5-point Likert scale did not sufficiently reflect how the participants were feeling and so they may have found it challenging to pick an option that best represented their thoughts about the experience. It is also possible that the conflict of opinion may not be paradoxical at all if the feedback came from different participants. Since the data has been pooled and generalised for convenience and analysis, it would have to be re-evaluated in finer detail to determine exactly what story the data is telling. 

Overall however, it seems (based on the relatively high average scores) that users were generally happy with the experience, but would like to see some improvements in the software in the future.  

_**Objective 3: To implement a system that will help users remember what they have learnt**_ 

The results obtained from participants would suggest that this objective has been met. 14 out of the 15 participants scored higher on their second attempt when answering the quiz questions, compared to the first attempt. The learning section of the questionnaire also scored relatively high, achieving an average score of 4.1. This implies that participants were happy to admit that learning had taken place as a direct result of using the application. That said, there are several factors that should also be taken into consideration.  

Firstly, the quiz questions were not randomised. Each time the participants took the test, the exact same questions were presented in exactly the same order as before. The multiple choice boxes were also arranged in the same order. It is therefore possible that participants were simply recalling the location of the answer buttons rather than the content contained within them. If this is the case, it would still allow them to score highly on the test, without really having learnt anything about culture.  

As shown in the box-plot presented earlier, the 30 minute gap between quiz attempts was not kept constant, but changed from participant to participant. When first looking at the data, the range can be seen to be 115 minutes. This is a very large spread of values. However, from the box-plot it can be seen that the data point corresponding to 120 minutes can be considered an outlier (of which there is only one). If we eliminate this from our analysis, we obtain a new range of 55. This is significantly lower than before, however the data still seems quite spread out. If we look at the inter-quartile range however, we can see that it drops to around 25 minutes with boundaries roughly centring around the median of 27 minutes. It therefore follows that 50% of the data collected falls within this inter-quartile range. Given that this range if fairly narrow, it can be said that at least half of the data falls within an acceptable range for the research to be considered fair.  

### COVID-19: 

The Coronavirus pandemic brought with it a number of challenges. Most notably, it prevented the team meeting in person and discussing ideas face-to-face. One of the issues this posed with regards to meetings is that people are generally less likely to miss a meeting that is scheduled to be in person. This is because meeting in person often requires forward planning, more time and an increased amount of effort both to arrange and in a logistical sense. Due to Coronavirus restrictions, all meetings were held online through Microsoft Teams. Attendance in these meetings was much lower than what would be expected from a face-to-face meeting - in some cases, only 2 Team members were present. With communication playing a critical role in the AGILE methodology, this is likely to have negatively impacted our progress in this project.  

In addition to the lack of attendance, there is also the problem in the way team members can interact with one another. If team members choose to keep their cameras off (and in some cases their microphones too) it is difficult to people’s reactions to ideas or feelings towards particular discussion points. On a few occasions, it was also apparent that one or two team members were entirely distracted and not focused on the call at all. This would unlikely be the case during a face-to-face meeting as there is less room for distraction. With cameras off, team members were free to work on other projects if they so wished and perhaps may choose to listen to the occasional comment being made every so often. This was deduced on several occasions where a question was asked to a team member and would have to be repeated because they were not paying attention.  

One of the benefits of meeting in person is that all members of the team can have their work in front of them and an overview of the entire project can be seen in one snapshot. Teams is somewhat limited in this sense as only one member of the Team was able to share their work at any given time. The big picture was therefore not explicitly displayed in front of the team but rather had to be mentally pieced together. 

The Coronavirus restrictions also had an effect on our ability to perform testing and gather feedback. Ideally, the team would have liked to go around to various people and present them with a laptop that already had the feedback survey displayed. People could then spend a few minutes filling it out on the spot and feedback could be obtained immediately. Unfortunately, this was not possible. Instead, links were sent out to a Google Form, which contained the survey and many of the participant did not get back to us with their feedback. Additionally, we were unable to focus solely on our target audience – namely, university students – since we were not able to meet people around campus with the hope of getting some feedback from them. This has resulted in making it a little more challenging to determine whether all the objectives have been met – since we have little feedback from our target audience.  

### Social and Ethical Implications: 

All new technology brings with it social and ethical issues that must be considered. In this project we set out to create an application that would educate people about the various cultures from around the world. In order to keep the quizzes short and concise, we had to narrow down the number of questions we could ask on a particular culture. One problem with this is that we are effectively condensing an entire culture down to just a few questions, which may not be entirely representative of the culture as a whole. Cultures are complex an even capturing the key aspects of a given culture often requires layers of detail.  

Another issue that could arise is if someone using the application is unable to find the culture that they associate themselves with. This could potentially offend some people and may even deter them from using the application altogether.  

### Improvements and Future Work: 

If were to undertake this project again, there are a number of things that we would do differently. The first of these is related to the client’s role in the development process. In this project we conducted just one evaluation with the clients, which took place towards the end of the project. This is not what we had initially planned. The AGILE methodology explicitly states the importance of including the client in the development process from the beginning and working with the team at all stages of development - I.e. a user centred approach. If we were to do this project again we would first gather information from our target audience and find out exactly what it is that they would like from us. We would then work with selected members to help guide us through the various stages of the project and ensure that we are delivering something that the client can actually use and has asked for. If we had access to the feedback we gained at the end of the project earlier on, we may have been able to improve some of the areas that were outlined by the clients before submitting the final product.  

Based on our user feedback, there are a number of changes and improvements we would wish to make to our application in the future. The first of these is with regards to the way in which questions are presented. At present, questions appear in the same order and multiple-choice answers appear in the same locations. An improvement on this would be to have questions appear randomly to ensure users aren’t simply remembering the pattern of the answers and are genuinely putting in the effort to actively recall the information.  

Another area we would like to improve on is the engagement aspect. Users informed us that the application was missing a competitive angle. This could perhaps be in the form of a timer on screen for users to compete against themselves, or perhaps even to introduce competition between different users by having them compete with one-another in real-time. Another way in which the application could be more engaging is to have a variety of activities rather than just the one quiz. There are numerous serious play implementations that could be included in this application, and it would be worth exploring these in future versions. Users may also want the option to choose which cultures to learn about rather than having the application decide for them. It may be beneficial to allow users to change the settings of the application to allow them to switch between choosing their own cultures to learn about and having the application choose for them.  

Other improvements are related to different design aspects of the application. For instance, we would like to condense the factual information presented to users after each question down to a minimum as the current length of the content is somewhat off-putting to our users. Users also wanted the background pictures to reflect the content of the specific question they were on. Currently we are using a generic background relating to the culture in question. There is also no way for users to undo an answer at present. It may be worth introducing a two-step process where users must first select an answer and then click a button to submit it. However, it may be that this process becomes too tedious for users and may negate the benefits gained from having a fast-paced game. We would like to have discussions with our users on this matter before deciding where or not it should be implemented.  

We also asked our users whether they would like to see a mobile version of the application. This was not included in the result as we later realised that it was not phrased with the positive spin required to ensure that high scores corresponded to positive feedback. However, when users were asked whether they would prefer a mobile version of the app, users were fairly neutral in the feelings towards this awarding it an average score of 3.1. Moving forward we would like to do a more in-depth study into the benefits of a mobile app to our users. For instance, it may be worth looking into the times of the day when applications such as ours are being used by our users. If it is generally on the move such as during commutes, it would be beneficial to produce a mobile version that would be accessible to a wider range of people (since more people use their phones while commuting than laptops).  
